[manager]
ip = 10.11.4.183
port = 1514
plugin = sahara
cluster_size = 1
flavor_id = 89789ded-183b-4a75-b7e9-5467d6fc944f 
image_id = 2494ff7c-7c6a-40b4-b57b-f54a3df112f3
bigsea_username = testuser
bigsea_password = aB123456

[plugin]
opportunistic = True
args = hdfs://10.11.4.155/user/ubuntu/BTR/input/test/ hdfs://10.11.4.155/user/ubuntu/BTR/output/prep_n/ hdfs://10.11.4.155/user/ubuntu/BTR/output/routestop_n/
dependencies = com.databricks:spark-csv_2.10:1.5.0
main_class = 
job_template_name = BTR
job_binary_name = BTR
job_binary_url = hdfs://10.11.4.155/user/ubuntu/BTR/preprocessing_subsequent_stops.py
input_ds_id = 
output_ds_id = 
plugin_app = spark_progress
expected_time = 100
collect_period = 5
openstack_plugin = spark
plugin_version = 2.1.0
job_type = Spark
cluster_id = 
master_ng = 6e84dd39-f672-4145-9c8f-8ffe984c0c1f 
slave_ng = 413b3782-9428-47a0-b4e5-15e223596f4e
opportunistic_slave_ng = 413b3782-9428-47a0-b4e5-15e223596f4e
net_id = 64ee4355-4d7f-4170-80b4-5e8348af6a61

[scaler]
starting_cap = 80
scaler_plugin = progress-error
actuator = service
metric_source = monasca
application_type = os_generic
check_interval = 10
trigger_down = 10
trigger_up = 10
min_cap = 20
max_cap = 100
actuation_size = 15
metric_rounding = 2
